{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKJcLnh6jJNj4fTG4fNxLI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himoyuzuki/colab_analytics/blob/main/%E6%96%B0%E5%9E%8B%E3%82%B3%E3%83%AD%E3%83%8A%E3%82%A6%E3%82%A4%E3%83%AB%E3%82%B9%E6%84%9F%E6%9F%93%E7%97%87%E3%81%AE%E5%A0%B1%E5%91%8A%E6%95%B0%E6%8E%A8%E7%A7%BB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新型コロナウイルス感染症の報告数推移\n",
        "\n",
        "2020年に世界中でパンデミックを引き落とした新型コロナウイルス感染症は、2025年、人々の記憶から急速に忘却されつつあり、「ニューノーマル」という言葉すら無くなるほど日常に回帰した。\n",
        "<br>\n",
        "しかし新型株が登場するなど、その脅威が消え去ったわけではない。忘れないためにも、感染者数の推移を定点観測する場所があっても良いのではないかと思い、このノートブックを作成する。"
      ],
      "metadata": {
        "id": "UITkLaOUVa_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install camelot-py\n",
        "!pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB3q2B2wWM22",
        "outputId": "596ff925-b0d7-4554-f0fd-03df839f4faf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting camelot-py\n",
            "  Downloading camelot_py-1.0.9-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from camelot-py) (8.2.1)\n",
            "Requirement already satisfied: chardet>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py) (5.2.0)\n",
            "Requirement already satisfied: numpy>=1.26.1 in /usr/local/lib/python3.12/dist-packages (from camelot-py) (2.0.2)\n",
            "Requirement already satisfied: openpyxl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py) (3.1.5)\n",
            "Collecting pdfminer-six>=20240706 (from camelot-py)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pypdf<6.0,>=4.0 (from camelot-py)\n",
            "  Downloading pypdf-5.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pandas>=2.2.2 in /usr/local/lib/python3.12/dist-packages (from camelot-py) (2.2.2)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py) (0.9.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.7.0.68 in /usr/local/lib/python3.12/dist-packages (from camelot-py) (4.12.0.88)\n",
            "Collecting pypdfium2>=4 (from camelot-py)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py) (11.3.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl>=3.1.0->camelot-py) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.2->camelot-py) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.2->camelot-py) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.2->camelot-py) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer-six>=20240706->camelot-py) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer-six>=20240706->camelot-py) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.2->camelot-py) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py) (2.22)\n",
            "Downloading camelot_py-1.0.9-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.9.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pypdf, pdfminer-six, camelot-py\n",
            "Successfully installed camelot-py-1.0.9 pdfminer-six-20250506 pypdf-5.9.0 pypdfium2-4.30.0\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfplumber\n",
            "Successfully installed pdfplumber-0.11.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OJYaoGkIVLc2"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import io\n",
        "import re\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import camelot\n",
        "import pdfplumber\n",
        "\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 厚労省のページから、5類移行後の感染者数推移を取得\n",
        "# 詳細: https://www.mhlw.go.jp/stf/seisakunitsuite/bunya/0000121431_00086.html\n",
        "BASE_URLS = [\"https://www.mhlw.go.jp/stf/seisakunitsuite/bunya/0000121431_00438.html\",  # 2023年\n",
        "             \"https://www.mhlw.go.jp/stf/seisakunitsuite/bunya/0000121431_00461.html\",  # 2024年\n",
        "             \"https://www.mhlw.go.jp/stf/seisakunitsuite/bunya/0000121431_00474.html\"  # 2025年\n",
        "             ]\n",
        "\n",
        "# ページから PDF リンクを取得\n",
        "pdf_links = []\n",
        "for BASE_URL in BASE_URLS:\n",
        "    resp = requests.get(BASE_URL)\n",
        "    resp.raise_for_status()\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"]\n",
        "        if href.endswith(\".pdf\") and \"content\" in href:\n",
        "            if href.startswith(\"http\"):\n",
        "                pdf_links.append(href)\n",
        "            else:\n",
        "                pdf_links.append(\"https://www.mhlw.go.jp\" + href)\n",
        "\n",
        "print(f\"Found {len(pdf_links)} PDF files\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h39SGUTNWGMg",
        "outputId": "9cfee689-95b4-4b8e-abec-699d3b818109"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 116 PDF files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_start_date(period_str, year_hint):\n",
        "    \"\"\"\n",
        "    '8月4日～8月10日' のような文字列から開始日を抽出し datetime に変換\n",
        "    year_hint: '2025年第32週' のような列から取得した西暦を補完\n",
        "    Args:\n",
        "        period_str (str): 期間を示す文字列（例: '8月4日～8月10日', '12月26日～1月1日'）。\n",
        "        year_hint (int or str): 週番号を含む年のヒント（例: 2024）。この週が属する西暦です。\n",
        "\n",
        "    Returns:\n",
        "        pd.Timestamp or None: 抽出された開始日のPandas Timestampオブジェクト。\n",
        "                              形式が一致しない場合やpd.NAの場合はNone。\n",
        "    \"\"\"\n",
        "    if pd.isna(period_str):\n",
        "        return None\n",
        "\n",
        "    # 日付を抽出\n",
        "    m = re.match(\n",
        "        r\"([0-9０-９]+)月([0-9０-９]+)日[～~](\\s*[0-9０-９]+)月([0-9０-９]+)日\",\n",
        "        str(period_str)\n",
        "    )\n",
        "    if not m:\n",
        "        return None\n",
        "\n",
        "    start_month = int(m.group(1))\n",
        "    start_day = int(m.group(2))\n",
        "    end_month = int(m.group(3))\n",
        "\n",
        "    year = int(year_hint)\n",
        "\n",
        "    # 期間が年度をまたぐかどうかを判定し、必要に応じて年を調整\n",
        "    if start_month == 12 and end_month == 1:\n",
        "        year -= 1\n",
        "\n",
        "    return pd.Timestamp(year=int(year), month=start_month, day=start_day)"
      ],
      "metadata": {
        "id": "NpcomttukV8R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 各PDFから都道府県別データを抽出\n",
        "all_dfs = []\n",
        "for link in pdf_links:\n",
        "    print(\"Processing:\", link)\n",
        "    try:\n",
        "        # PDFをダウンロード\n",
        "        pdf_resp = requests.get(link)\n",
        "        pdf_resp.raise_for_status()\n",
        "\n",
        "        # 一時ファイル扱いにして読み込む\n",
        "        with open(\"temp.pdf\", \"wb\") as f:\n",
        "            f.write(pdf_resp.content)\n",
        "\n",
        "        # 2ページ目を抽出\n",
        "        tables = camelot.read_pdf(\"temp.pdf\", pages=\"2\")\n",
        "        if len(tables) == 0:\n",
        "            print(\"No tables found\")\n",
        "            continue\n",
        "        df = tables[0].df\n",
        "\n",
        "        # 日付情報を取得\n",
        "        with open(\"temp.pdf\", \"rb\") as f:\n",
        "            with pdfplumber.open(f) as pdf:\n",
        "                page2_text = pdf.pages[1].extract_text()\n",
        "\n",
        "        week_info_match = re.search(r\"(\\d{4})年第\\s*(\\d+)\\s*週\\(?([0-9月日〜\\s\\-～]+)\\)?\", page2_text)\n",
        "        if week_info_match:\n",
        "            year = int(week_info_match.group(1))\n",
        "            week_no = int(week_info_match.group(2))\n",
        "            period = week_info_match.group(3).strip() if week_info_match.group(3) else None\n",
        "            week_label = f\"{year}年第{week_no}週\"\n",
        "        else:\n",
        "            print(\"No week info found\")\n",
        "            week_label, period = None, None\n",
        "\n",
        "        # DataFrame調整\n",
        "        df.columns = ['都道府県', '報告数', '定点あたり']\n",
        "\n",
        "        if  df.shape[0] < 51:  # 「昨年同月比」レコードが追加されているか否か\n",
        "            df = df.drop([0, 1, 49]).reset_index(drop=True)\n",
        "        else:\n",
        "            df = df.drop([0, 1, 49, 50]).reset_index(drop=True)\n",
        "\n",
        "        df = df.replace(\"\\n\", \"\", regex=True)\n",
        "        df[\"報告数\"] = df[\"報告数\"].str.replace(\",\", \"\").astype(int)\n",
        "        df[\"定点あたり\"] = df[\"定点あたり\"].astype(float)\n",
        "        df[\"週\"] = week_label\n",
        "        df[\"期間\"] = period\n",
        "\n",
        "        all_dfs.append(df)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Failed to process\", link, e)\n",
        "\n",
        "# 結合\n",
        "if all_dfs:\n",
        "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
        "    final_df[\"year\"] = final_df[\"週\"].str.extract(r\"(\\d{4})\").astype(int)\n",
        "    final_df[\"開始日\"] = final_df.apply(lambda x: extract_start_date(x[\"期間\"], x[\"year\"]), axis=1)\n",
        "    print(final_df.head())\n",
        "else:\n",
        "    final_df = pd.DataFrame()\n",
        "    print(\"No tables extracted\")\n",
        "\n",
        "# 保存\n",
        "final_df.to_csv(\"covid_prefecture_timeseries.csv\", index=False, encoding=\"utf-8-sig\")"
      ],
      "metadata": {
        "id": "LyeKc_g7WeOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 都道府県別に複数レコードがあるため、ここでは全国合計を例示\n",
        "df_total = final_df.groupby(\"開始日\", as_index=False)[\"報告数\"].sum().sort_values(\"開始日\")\n",
        "\n",
        "# Plotlyで折れ線グラフ\n",
        "fig = px.line(df_total, x=\"開始日\", y=\"報告数\", title=\"新型コロナ報告数（週次・全国合計）\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "oc13O6-ZWfph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PcdkDU7_uYyE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}